{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a33ca0",
   "metadata": {},
   "source": [
    "# Inference Testing with Roboflow, Azure & Swiss AI Platform\n",
    "\n",
    "1. **Roboflow**: Fine-tuned instance segmentation model for detecting objects in images\n",
    "2. **Azure GPT-4.1**: Multimodal model that can analyze images and respond to text prompts\n",
    "3. **Swisscom Llama 3.2 Vision**: Another multimodal model capable of analyzing images and text prompts\n",
    "\n",
    "(same image and similar prompts across all models to compare their responses and capabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f77c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import time\n",
    "import base64\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from roboflow import Roboflow\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if .env file exists, if not create a placeholder\n",
    "try:\n",
    "    with open('.env', 'r') as f:\n",
    "        env_content = f.read()\n",
    "    print(\"Found .env file\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"No .env file found. Please create a .env file with the required API keys.\")\n",
    "\n",
    "# Define the prompt to use for both text-based models\n",
    "prompt = \"Analyze this image and find RED and GREY marketing poster placeholders. Describe their content, placement, and potential audience.\"\n",
    "\n",
    "# Define image path\n",
    "img_path = \"Stellenbilder-no-augs/test/66229_jpg.rf.d4ba322d0570bb4e119115e226dcb558.jpg\"\n",
    "\n",
    "# Create output directory for results if it doesn't exist\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# Helper function to display images\n",
    "def display_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(np.array(img))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.show()\n",
    "    return img\n",
    "\n",
    "# Display the input image\n",
    "try:\n",
    "    img = display_image(img_path)\n",
    "    print(f\"Image loaded successfully from {img_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading image: {e}\")\n",
    "    # Try looking in the augmented folder instead\n",
    "    img_path = \"Stellenbilder-no-augs/test/66229_jpg.rf.d4ba322d0570bb4e119115e226dcb558.jpg\"\n",
    "    try:\n",
    "        img = display_image(img_path)\n",
    "        print(f\"Image loaded successfully from {img_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Still could not load image: {e}\")\n",
    "        print(\"Please ensure the image exists or update the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a61bd6",
   "metadata": {},
   "source": [
    "## 1. Roboflow Model: Instance Segmentation\n",
    "\n",
    "Using the fine-tuned Roboflow model to detect and segment objects in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Roboflow API key\n",
    "roboflow_api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n",
    "\n",
    "if not roboflow_api_key:\n",
    "    print(\"Roboflow API key not found in .env file. Skipping Roboflow model.\")\n",
    "else:\n",
    "    try:\n",
    "        \n",
    "        # Initialize Roboflow\n",
    "        rf = Roboflow(api_key=roboflow_api_key)\n",
    "        \n",
    "        # Access Roboflow workspace and project\n",
    "        project = rf.workspace(\"goldbach-neo-testspace\").project(\"stellenbilder\")\n",
    "        \n",
    "        # Get the model version\n",
    "        model = project.version(3).model  # Using version 3 directly\n",
    "        print(f\"Using Roboflow model: {project.name} version 3\")\n",
    "        \n",
    "        # Make prediction on the image\n",
    "        print(\"Running inference with Roboflow SDK...\")\n",
    "        result = model.predict(img_path, confidence=0.4).json()\n",
    "        \n",
    "        # Save the raw result\n",
    "        with open(\"output/roboflow_result.json\", \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        \n",
    "        # For visualization function\n",
    "        predictions = result  # This is already in the right format\n",
    "        \n",
    "        # Visualize the predictions\n",
    "        def visualize_predictions(image_path, predictions):\n",
    "            # Load image\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Drawing parameters\n",
    "            thickness = 2\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.5\n",
    "            font_thickness = 1\n",
    "            colors = {\n",
    "                \"DIG\": (255, 0, 0),        # Red\n",
    "                \"F12\": (0, 255, 0),       # Green\n",
    "                \"F200\": (0, 0, 255),      # Blue\n",
    "                \"F24\": (255, 255, 0),     # Yellow\n",
    "                \"F4\": (255, 0, 255),      # Magenta\n",
    "                \"Megaposter\": (0, 255, 255),  # Cyan\n",
    "                \"default\": (128, 128, 128) # Gray for any other class\n",
    "            }\n",
    "            \n",
    "            # Keep track of all classes\n",
    "            classes_found = set()\n",
    "            \n",
    "            # Draw predictions\n",
    "            for pred in predictions['predictions']:\n",
    "                # Get class name or default to 'unknown'\n",
    "                class_name = pred.get('class', 'unknown')\n",
    "                classes_found.add(class_name)\n",
    "                confidence = pred.get('confidence', 0)\n",
    "                \n",
    "                # Get bounding box\n",
    "                x = pred.get('x', 0)\n",
    "                y = pred.get('y', 0)\n",
    "                width = pred.get('width', 0)\n",
    "                height = pred.get('height', 0)\n",
    "                \n",
    "                # Calculate box coordinates\n",
    "                x1 = int(x - width/2)\n",
    "                y1 = int(y - height/2)\n",
    "                x2 = int(x + width/2)\n",
    "                y2 = int(y + height/2)\n",
    "                \n",
    "                # Get color for class\n",
    "                color = colors.get(class_name, colors['default'])\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "                \n",
    "                # Add label with confidence\n",
    "                label = f\"{class_name}: {confidence:.2f}\"\n",
    "                (label_width, label_height), _ = cv2.getTextSize(label, font, font_scale, font_thickness)\n",
    "                cv2.rectangle(image, (x1, y1 - label_height - 5), (x1 + label_width, y1), color, -1)\n",
    "                cv2.putText(image, label, (x1, y1 - 5), font, font_scale, (255, 255, 255), font_thickness)\n",
    "                \n",
    "                # Draw mask if available\n",
    "                if 'points' in pred and isinstance(pred['points'], dict):\n",
    "                    points = pred['points']\n",
    "                    for k, point_list in points.items():\n",
    "                        # Convert points to NumPy array for drawing\n",
    "                        pts = np.array(point_list, np.int32)\n",
    "                        pts = pts.reshape((-1, 1, 2))\n",
    "                        # Draw the polygon\n",
    "                        cv2.polylines(image, [pts], True, color, 2)\n",
    "            \n",
    "            # Create a legend\n",
    "            legend_y = 30\n",
    "            for class_name in sorted(classes_found):\n",
    "                color = colors.get(class_name, colors['default'])\n",
    "                cv2.rectangle(image, (10, legend_y-15), (25, legend_y), color, -1)\n",
    "                cv2.putText(image, class_name, (30, legend_y), font, font_scale*1.2, color, font_thickness)\n",
    "                legend_y += 25\n",
    "                \n",
    "            # Display image\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            plt.title(\"Roboflow Instance Segmentation Results\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Save the visualization\n",
    "            output_img = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(\"output/roboflow_prediction.jpg\", output_img)\n",
    "            print(\"Saved visualization to output/roboflow_prediction.jpg\")\n",
    "            \n",
    "            return classes_found\n",
    "        \n",
    "        # Visualize and get class information\n",
    "        classes = visualize_predictions(img_path, predictions)\n",
    "        print(f\"Classes detected: {', '.join(classes)}\")\n",
    "        print(f\"Found {len(predictions['predictions'])} objects in the image.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with Roboflow model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c008cfaa",
   "metadata": {},
   "source": [
    "## 2. Azure GPT-4.1: Multimodal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ea284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Azure credentials\n",
    "azure_api_key = os.getenv(\"AZURE_API_KEY\")\n",
    "azure_url = os.getenv(\"AZURE_URL\") or \"https://aihubgptrag0925019255.openai.azure.com/\"\n",
    "\n",
    "# Azure configuration\n",
    "endpoint = azure_url\n",
    "model_name = \"gpt-4.1\"\n",
    "deployment = \"gpt-4.1-workshop\"\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "if not azure_api_key:\n",
    "    print(\"Azure API key not found in .env file. Skipping Azure GPT-4.1 model.\")\n",
    "else:\n",
    "    try:\n",
    "        from openai import AzureOpenAI\n",
    "        \n",
    "        # Function to encode image to base64\n",
    "        def encode_image(image_path):\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        \n",
    "        # Initialize Azure OpenAI client with the updated parameters\n",
    "        client = AzureOpenAI(\n",
    "            api_version=api_version,\n",
    "            azure_endpoint=endpoint,\n",
    "            api_key=azure_api_key,\n",
    "        )\n",
    "        \n",
    "        # Encode the image\n",
    "        base64_image = encode_image(img_path)\n",
    "        \n",
    "        # Prepare the message with image and text\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that analyzes images in detail.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(\"Sending request to Azure GPT-4.1...\")\n",
    "        \n",
    "        # Call the model using the updated parameters\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            max_completion_tokens=800,  # Changed from max_tokens\n",
    "            temperature=0.7,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            model=deployment  # Using the deployment name instead of model\n",
    "        )\n",
    "        \n",
    "        # Print the response\n",
    "        print(\"\\n===== Azure GPT-4.1 Response =====\\n\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"\\n==================================\\n\")\n",
    "        \n",
    "        # Save the response to a file\n",
    "        with open(\"output/azure_gpt4_response.txt\", \"w\") as f:\n",
    "            f.write(response.choices[0].message.content)\n",
    "        print(\"Saved Azure GPT-4.1 response to output/azure_gpt4_response.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with Azure GPT-4.1 model: {e}\")\n",
    "        print(\"Try checking if the deployment name 'gpt-4.1-workshop' is correct for your Azure account.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9022c0",
   "metadata": {},
   "source": [
    "## 3. Swisscom Llama 3.2 Vision: Multimodal Analysis\n",
    "\n",
    "Using Swisscom's Llama 3.2 vision model to analyze the image with a text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Swisscom credentials\n",
    "swisscom_api_key = os.getenv(\"SWISSCOM_AI_KEY\")\n",
    "swisscom_base_url = os.getenv(\"SWISSCOM_AI_BASEURL\")\n",
    "swisscom_vision_model = os.getenv(\"SWISSCOM_AI_VISION_MODEL\", \"meta/llama-3.2-90b-vision-instruct\")\n",
    "\n",
    "if not swisscom_api_key or not swisscom_base_url:\n",
    "    print(\"Swisscom API key or base URL not found in .env file. Skipping Swisscom Llama 3.2 model.\")\n",
    "else:\n",
    "    try:\n",
    "        # Function to convert image to base64\n",
    "        def image_to_base64(image_path):\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        \n",
    "        # Convert our image to base64\n",
    "        image_base64 = image_to_base64(img_path)\n",
    "        \n",
    "        # Prepare the request to Swisscom API\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {swisscom_api_key}\"\n",
    "        }\n",
    "        \n",
    "        # Using the message format similar to vision_transformation.py\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Prepare payload\n",
    "        payload = {\n",
    "            \"model\": swisscom_vision_model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 800,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        print(f\"Sending request to Swisscom Llama 3.2 Vision ({swisscom_vision_model})...\")\n",
    "        \n",
    "        # Call the Swisscom API\n",
    "        response = requests.post(\n",
    "            f\"{swisscom_base_url}/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload\n",
    "        )\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            llama_response = response_data['choices'][0]['message']['content']\n",
    "            \n",
    "            # Print the response\n",
    "            print(\"\\n===== Swisscom Llama 3.2 Response =====\\n\")\n",
    "            print(llama_response)\n",
    "            print(\"\\n==================================\\n\")\n",
    "            \n",
    "            # Save the response to a file\n",
    "            with open(\"output/swisscom_llama_response.txt\", \"w\") as f:\n",
    "                f.write(llama_response)\n",
    "            print(\"Saved Swisscom Llama 3.2 response to output/swisscom_llama_response.txt\")\n",
    "            \n",
    "            # Print token usage if available\n",
    "            if 'usage' in response_data:\n",
    "                usage = response_data['usage']\n",
    "                print(f\"Token usage: {usage.get('total_tokens', 'N/A')} total tokens \")\n",
    "                print(f\"({usage.get('prompt_tokens', 'N/A')} prompt, {usage.get('completion_tokens', 'N/A')} completion)\")\n",
    "        else:\n",
    "            print(f\"Error: Received status code {response.status_code} from Swisscom API\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error with Swisscom Llama 3.2 model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f7d96",
   "metadata": {},
   "source": [
    "## Model Comparison and Analysis\n",
    "\n",
    "1. **Roboflow** provides localization and classification of objects through instance segmentation.\n",
    "2. **Azure GPT-4.1** offers detailed text analysis and understanding of the image content.\n",
    "3. **Swisscom Llama 3.2** provides another perspective on image analysis with potentially different capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f0df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    azure_response = \"\"\n",
    "    llama_response = \"\"\n",
    "    \n",
    "    # Try to load Azure response\n",
    "    try:\n",
    "        with open(\"output/azure_gpt4_response.txt\", \"r\") as f:\n",
    "            azure_response = f.read()\n",
    "    except FileNotFoundError:\n",
    "        azure_response = \"Azure GPT-4.1 response not available\"\n",
    "    \n",
    "    # Try to load Swisscom response\n",
    "    try:\n",
    "        with open(\"output/swisscom_llama_response.txt\", \"r\") as f:\n",
    "            llama_response = f.read()\n",
    "    except FileNotFoundError:\n",
    "        llama_response = \"Swisscom Llama 3.2 response not available\"\n",
    "    \n",
    "    # Create a side-by-side comparison in markdown\n",
    "    from IPython.display import Markdown, display\n",
    "    \n",
    "    comparison_md = \"\"\"\n",
    "    # Response Comparison\n",
    "    \n",
    "    | Azure GPT-4.1 | Swisscom Llama 3.2 |\n",
    "    |---------------|--------------------|\n",
    "    | {} | {} |\n",
    "    \"\"\".format(azure_response.replace('\\n', '<br>'), llama_response.replace('\\n', '<br>'))\n",
    "    \n",
    "    display(Markdown(comparison_md))\n",
    "    \n",
    "    # Save comparison to a file\n",
    "    with open(\"output/model_comparison.md\", \"w\") as f:\n",
    "        f.write(comparison_md)\n",
    "    print(\"Saved comparison to output/model_comparison.md\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating comparison: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f05088",
   "metadata": {},
   "source": [
    "## 4. Systematic Comparison: Classification and Location Extraction\n",
    "\n",
    "Analyzes multiple images from our test set using both GPT-4.1 and Llama 3.2 models to:\n",
    "\n",
    "1. Detect red and grey marketing poster placeholders\n",
    "2. Extract their locations\n",
    "3. Generate structured output\n",
    "4. Compare the results between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6aeb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structured prompt for both models\n",
    "structured_prompt = \"\"\"\n",
    "Analyze this image and identify all RED and GREY marketing poster placeholders.\n",
    "\n",
    "Provide your answer in the following JSON format only:\n",
    "{\n",
    "    \"red_placeholders\": {\n",
    "        \"count\": <number of red placeholders>,\n",
    "        \"locations\": [\"<description of location 1>\", \"<description of location 2>\", ...]\n",
    "    },\n",
    "    \"grey_placeholders\": {\n",
    "        \"count\": <number of grey placeholders>,\n",
    "        \"locations\": [\"<description of location 1>\", \"<description of location 2>\", ...]\n",
    "    },\n",
    "    \"has_red_placeholder\": <true/false>,\n",
    "    \"has_grey_placeholder\": <true/false>\n",
    "}\n",
    "\n",
    "Be sure to accurately count the placeholders and describe their relative positions in the image (e.g., \"top left\", \"bottom right\", \"center\", etc.). Only include placeholders that are clearly visible in the image.\n",
    "\"\"\"\n",
    "\n",
    "# Extract JSON from model responses with error handling\n",
    "def extract_json_from_response(text):\n",
    "    # Try to find JSON content between triple backticks\n",
    "    import re\n",
    "    import json\n",
    "    \n",
    "    # Try the most common formats\n",
    "    json_patterns = [\n",
    "        r'```json\\s*({.*?})\\s*```',  # JSON with json tag\n",
    "        r'```\\s*({.*?})\\s*```',      # JSON with just backticks\n",
    "        r'{\\s*\"red_placeholders\".*?}\\s*}(?=\\s|$)',  # Raw JSON pattern\n",
    "    ]\n",
    "    \n",
    "    # Try each pattern\n",
    "    for pattern in json_patterns:\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        if matches:\n",
    "            try:\n",
    "                return json.loads(matches[0])\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # If no pattern worked, try to extract the entire JSON object\n",
    "    try:\n",
    "        # Find the opening brace\n",
    "        start_idx = text.find('{')\n",
    "        if start_idx != -1:\n",
    "            # Count opening and closing braces to find the matching closing brace\n",
    "            brace_count = 0\n",
    "            for i in range(start_idx, len(text)):\n",
    "                if text[i] == '{':\n",
    "                    brace_count += 1\n",
    "                elif text[i] == '}':\n",
    "                    brace_count -= 1\n",
    "                    if brace_count == 0:\n",
    "                        # Found matching closing brace\n",
    "                        json_str = text[start_idx:i+1]\n",
    "                        return json.loads(json_str)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Last resort: default values if parsing fails\n",
    "    print(\"Could not extract JSON from response, using default values\")\n",
    "    return {\n",
    "        \"red_placeholders\": {\"count\": 0, \"locations\": []},\n",
    "        \"grey_placeholders\": {\"count\": 0, \"locations\": []},\n",
    "        \"has_red_placeholder\": False,\n",
    "        \"has_grey_placeholder\": False,\n",
    "        \"parsing_error\": True\n",
    "    }\n",
    "\n",
    "# Process a single image with both models and return structured results\n",
    "def process_image_with_models(image_path, azure_client, swisscom_api_key, swisscom_base_url, swisscom_vision_model):\n",
    "    print(f\"Processing {image_path}...\")\n",
    "    \n",
    "    # Image base64 encoding (shared between models)\n",
    "    base64_image = encode_image(image_path)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Azure GPT-4.1\n",
    "    if azure_client is not None:\n",
    "        try:\n",
    "            # Prepare the message with image and text\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a specialized computer vision assistant that only responds in JSON format.\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": structured_prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Call the model\n",
    "            response = azure_client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                max_completion_tokens=800,\n",
    "                temperature=0.3,  # Lower temperature for more deterministic outputs\n",
    "                top_p=1.0,\n",
    "                frequency_penalty=0.0,\n",
    "                presence_penalty=0.0,\n",
    "                model=deployment\n",
    "            )\n",
    "            \n",
    "            # Extract JSON from response\n",
    "            gpt4_text = response.choices[0].message.content\n",
    "            results['gpt4'] = {\n",
    "                'raw_response': gpt4_text,\n",
    "                'parsed': extract_json_from_response(gpt4_text)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with Azure GPT-4.1 on {image_path}: {e}\")\n",
    "            results['gpt4'] = {\n",
    "                'raw_response': f\"Error: {str(e)}\",\n",
    "                'parsed': {\n",
    "                    \"red_placeholders\": {\"count\": 0, \"locations\": []},\n",
    "                    \"grey_placeholders\": {\"count\": 0, \"locations\": []},\n",
    "                    \"has_red_placeholder\": False,\n",
    "                    \"has_grey_placeholder\": False,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    # 2. Swisscom Llama 3.2\n",
    "    if swisscom_api_key and swisscom_base_url:\n",
    "        try:\n",
    "            # Prepare the request to Swisscom API\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {swisscom_api_key}\"\n",
    "            }\n",
    "            \n",
    "            # Using the message format\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a specialized computer vision assistant that only responds in JSON format.\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": structured_prompt},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                    ]\n",
    "                }]\n",
    "            \n",
    "            # Prepare payload\n",
    "            payload = {\n",
    "                \"model\": swisscom_vision_model,\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0.3,  # Lower temperature for more deterministic outputs\n",
    "                \"max_tokens\": 800,\n",
    "                \"stream\": False}\n",
    "            \n",
    "            # Call the Swisscom API\n",
    "            response = requests.post(\n",
    "                f\"{swisscom_base_url}/chat/completions\",\n",
    "                headers=headers,\n",
    "                json=payload)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                response_data = response.json()\n",
    "                llama_text = response_data['choices'][0]['message']['content']\n",
    "                results['llama'] = {\n",
    "                    'raw_response': llama_text,\n",
    "                    'parsed': extract_json_from_response(llama_text)}\n",
    "            else:\n",
    "                raise Exception(f\"Status code: {response.status_code}, Response: {response.text}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with Swisscom Llama 3.2 on {image_path}: {e}\")\n",
    "            results['llama'] = {\n",
    "                'raw_response': f\"Error: {str(e)}\",\n",
    "                'parsed': {\n",
    "                    \"red_placeholders\": {\"count\": 0, \"locations\": []},\n",
    "                    \"grey_placeholders\": {\"count\": 0, \"locations\": []},\n",
    "                    \"has_red_placeholder\": False,\n",
    "                    \"has_grey_placeholder\": False,\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all test images and collect results\n",
    "azure_client = None\n",
    "if azure_api_key:\n",
    "    try:\n",
    "        from openai import AzureOpenAI\n",
    "        # Initialize Azure OpenAI client\n",
    "        azure_client = AzureOpenAI(\n",
    "            api_version=api_version,\n",
    "            azure_endpoint=endpoint,\n",
    "            api_key=azure_api_key,\n",
    "        )\n",
    "        print(\"Azure client initialized successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Azure client: {e}\")\n",
    "\n",
    "# Create output directory for comparative results\n",
    "os.makedirs(\"output/comparison\", exist_ok=True)\n",
    "\n",
    "# Get all test images\n",
    "import glob\n",
    "test_images = glob.glob(\"Stellenbilder-no-augs/test/*.jpg\")\n",
    "print(f\"Found {len(test_images)} test images\")\n",
    "\n",
    "# Process each image with both models and collect results\n",
    "comparison_results = []\n",
    "\n",
    "# For demo purposes, limit to a few images if needed\n",
    "max_images = 5  # You can increase this or set to None for all images\n",
    "if max_images:\n",
    "    test_images = test_images[:max_images]\n",
    "    print(f\"Processing {len(test_images)} images...\")\n",
    "\n",
    "# Process each image\n",
    "all_results = {}\n",
    "for img_path in test_images:\n",
    "    # Extract image name from path\n",
    "    img_name = os.path.basename(img_path)\n",
    "    \n",
    "    # Process the image with both models\n",
    "    results = process_image_with_models(\n",
    "        img_path, \n",
    "        azure_client, \n",
    "        swisscom_api_key, \n",
    "        swisscom_base_url, \n",
    "        swisscom_vision_model\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    all_results[img_name] = results\n",
    "    \n",
    "    # Add to comparison table\n",
    "    if 'gpt4' in results:\n",
    "        gpt4_data = results['gpt4']['parsed']\n",
    "        comparison_results.append({\n",
    "            'image': img_name,\n",
    "            'model': 'GPT-4.1',\n",
    "            'red_count': gpt4_data['red_placeholders']['count'],\n",
    "            'grey_count': gpt4_data['grey_placeholders']['count'],\n",
    "            'has_red': gpt4_data['has_red_placeholder'],\n",
    "            'has_grey': gpt4_data['has_grey_placeholder'],\n",
    "            'red_locations': ', '.join(gpt4_data['red_placeholders']['locations']),\n",
    "            'grey_locations': ', '.join(gpt4_data['grey_placeholders']['locations'])\n",
    "        })\n",
    "    \n",
    "    if 'llama' in results:\n",
    "        llama_data = results['llama']['parsed']\n",
    "        comparison_results.append({\n",
    "            'image': img_name,\n",
    "            'model': 'Llama 3.2',\n",
    "            'red_count': llama_data['red_placeholders']['count'],\n",
    "            'grey_count': llama_data['grey_placeholders']['count'],\n",
    "            'has_red': llama_data['has_red_placeholder'],\n",
    "            'has_grey': llama_data['has_grey_placeholder'],\n",
    "            'red_locations': ', '.join(llama_data['red_placeholders']['locations']),\n",
    "            'grey_locations': ', '.join(llama_data['grey_placeholders']['locations'])\n",
    "        })\n",
    "\n",
    "# Save raw results to file\n",
    "with open('output/comparison/raw_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"Processed {len(test_images)} images with both models\")\n",
    "print(f\"Results saved to output/comparison/raw_results.json\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(comparison_results)\n",
    "\n",
    "# Save comparison to CSV\n",
    "df_comparison.to_csv('output/comparison/model_comparison.csv', index=False)\n",
    "\n",
    "# Display the comparison table\n",
    "from IPython.display import display\n",
    "print(\"\\nComparison Table:\")\n",
    "display(df_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c41836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison results\n",
    "\n",
    "# 1. Create a grouped bar chart for count comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Reshape data for easier plotting\n",
    "model_counts = pd.DataFrame()\n",
    "\n",
    "# Group by image and model, then calculate counts\n",
    "for img in df_comparison['image'].unique():\n",
    "    img_data = df_comparison[df_comparison['image'] == img]\n",
    "    \n",
    "    # Get data for each model\n",
    "    for model in img_data['model'].unique():\n",
    "        model_data = img_data[img_data['model'] == model]\n",
    "        \n",
    "        # Add to the dataframe\n",
    "        model_counts = pd.concat([model_counts, pd.DataFrame({\n",
    "            'Image': [img],\n",
    "            'Model': [model],\n",
    "            'Red Placeholders': [model_data['red_count'].values[0]],\n",
    "            'Grey Placeholders': [model_data['grey_count'].values[0]]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "# Reshape data for seaborn\n",
    "model_counts_melted = pd.melt(\n",
    "    model_counts, \n",
    "    id_vars=['Image', 'Model'], \n",
    "    value_vars=['Red Placeholders', 'Grey Placeholders'],\n",
    "    var_name='Placeholder Type', \n",
    "    value_name='Count'\n",
    ")\n",
    "\n",
    "# Create the grouped bar chart\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    data=model_counts_melted,\n",
    "    x='Image',\n",
    "    y='Count',\n",
    "    hue='Placeholder Type',\n",
    "    palette=['darkred', 'darkgrey'],\n",
    "    errorbar=None,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add a second grouping by model\n",
    "n_images = len(model_counts['Image'].unique())\n",
    "n_models = len(model_counts['Model'].unique())\n",
    "model_offsets = np.linspace(-0.2, 0.2, n_models)\n",
    "\n",
    "# Highlight model differences with patterns\n",
    "for i, (img, img_group) in enumerate(model_counts_melted.groupby('Image')):\n",
    "    for j, (model, model_group) in enumerate(img_group.groupby('Model')):\n",
    "        for k, (placeholder_type, type_group) in enumerate(model_group.groupby('Placeholder Type')):\n",
    "            # Add a model indicator\n",
    "            pattern = '//' if model == 'GPT-4.1' else 'xx'\n",
    "            plt.annotate(\n",
    "                model,\n",
    "                xy=(i + model_offsets[j], type_group['Count'].values[0] + 0.1),\n",
    "                xytext=(0, 5),\n",
    "                textcoords='offset points',\n",
    "                ha='center',\n",
    "                fontsize=8,\n",
    "                rotation=90,\n",
    "                color='black' if model == 'GPT-4.1' else 'darkblue'\n",
    "            )\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Comparison of Placeholder Counts: GPT-4.1 vs Llama 3.2', fontsize=16)\n",
    "plt.xlabel('Image', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Placeholder Type', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('output/comparison/placeholder_counts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Create a heatmap to compare model agreement\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Calculate agreement matrix\n",
    "agreement_data = []\n",
    "\n",
    "for img in df_comparison['image'].unique():\n",
    "    img_data = df_comparison[df_comparison['image'] == img]\n",
    "    \n",
    "    # Get data for each model\n",
    "    gpt_data = img_data[img_data['model'] == 'GPT-4.1']\n",
    "    llama_data = img_data[img_data['model'] == 'Llama 3.2']\n",
    "    \n",
    "    if not gpt_data.empty and not llama_data.empty:\n",
    "        # Calculate difference metrics\n",
    "        red_diff = abs(gpt_data['red_count'].values[0] - llama_data['red_count'].values[0])\n",
    "        grey_diff = abs(gpt_data['grey_count'].values[0] - llama_data['grey_count'].values[0])\n",
    "        \n",
    "        # Red placeholder agreement (boolean)\n",
    "        red_bool_agreement = gpt_data['has_red'].values[0] == llama_data['has_red'].values[0]\n",
    "        \n",
    "        # Grey placeholder agreement (boolean)\n",
    "        grey_bool_agreement = gpt_data['has_grey'].values[0] == llama_data['has_grey'].values[0]\n",
    "        \n",
    "        # Overall agreement score (lower is better)\n",
    "        agreement_score = red_diff + grey_diff + (0 if red_bool_agreement else 1) + (0 if grey_bool_agreement else 1)\n",
    "        \n",
    "        agreement_data.append({\n",
    "            'Image': img,\n",
    "            'Red Count Diff': red_diff,\n",
    "            'Grey Count Diff': grey_diff,\n",
    "            'Red Bool Agreement': red_bool_agreement,\n",
    "            'Grey Bool Agreement': grey_bool_agreement,\n",
    "            'Agreement Score': agreement_score\n",
    "        })\n",
    "\n",
    "# Create DataFrame for heatmap\n",
    "df_agreement = pd.DataFrame(agreement_data)\n",
    "\n",
    "# Pivot for heatmap format\n",
    "heatmap_data = df_agreement.set_index('Image')[['Red Count Diff', 'Grey Count Diff', 'Agreement Score']]\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, len(heatmap_data) * 0.8))\n",
    "sns.heatmap(\n",
    "    heatmap_data, \n",
    "    annot=True, \n",
    "    cmap='RdYlGn_r',  # Reversed so red means more disagreement\n",
    "    linewidths=0.5,\n",
    "    fmt=\".0f\")\n",
    "\n",
    "plt.title('Model Agreement Heatmap (Lower = Better Agreement)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('output/comparison/model_agreement.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aeb9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations with actual images and model annotations\n",
    "\n",
    "# Function to draw placeholder annotations on images\n",
    "def draw_annotations(image_path, model_results, model_name):\n",
    "    # Load and convert image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create a copy for drawing\n",
    "    annotated_img = img.copy()\n",
    "    \n",
    "    # Get image dimensions\n",
    "    h, w, _ = img.shape\n",
    "    \n",
    "    # Font settings\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.7\n",
    "    font_thickness = 2\n",
    "    \n",
    "    # Add title\n",
    "    cv2.putText(\n",
    "        annotated_img, \n",
    "        f\"{model_name} Analysis\", \n",
    "        (10, 30), \n",
    "        font, \n",
    "        1, \n",
    "        (255, 255, 255), \n",
    "        font_thickness + 1\n",
    "    )\n",
    "    \n",
    "    # Draw a semitransparent overlay for the legend\n",
    "    overlay = annotated_img.copy()\n",
    "    cv2.rectangle(overlay, (10, 40), (300, 120), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.5, annotated_img, 0.5, 0, annotated_img)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_y = 70\n",
    "    cv2.putText(annotated_img, f\"Red placeholders: {model_results['red_placeholders']['count']}\", \n",
    "                (20, legend_y), font, font_scale, (255, 50, 50), font_thickness)\n",
    "    legend_y += 30\n",
    "    cv2.putText(annotated_img, f\"Grey placeholders: {model_results['grey_placeholders']['count']}\", \n",
    "                (20, legend_y), font, font_scale, (180, 180, 180), font_thickness)\n",
    "    \n",
    "    # Draw approximate locations based on text descriptions (this is simplified)\n",
    "    # Map common location terms to image coordinates (as ratios of image dimensions)\n",
    "    location_map = {\n",
    "        'top left': (0.25, 0.25),\n",
    "        'top right': (0.75, 0.25),\n",
    "        'bottom left': (0.25, 0.75),\n",
    "        'bottom right': (0.75, 0.75),\n",
    "        'top': (0.5, 0.25),\n",
    "        'bottom': (0.5, 0.75),\n",
    "        'left': (0.25, 0.5),\n",
    "        'right': (0.75, 0.5),\n",
    "        'center': (0.5, 0.5),\n",
    "        'middle': (0.5, 0.5),\n",
    "        'upper left': (0.25, 0.25),\n",
    "        'upper right': (0.75, 0.25),\n",
    "        'lower left': (0.25, 0.75),\n",
    "        'lower right': (0.75, 0.75),\n",
    "    }\n",
    "    \n",
    "    # Helper function to guess position from text description\n",
    "    def guess_position(location_text, fallback=(0.5, 0.5)):\n",
    "        location_text = location_text.lower()\n",
    "        for key, pos in location_map.items():\n",
    "            if key in location_text:\n",
    "                return pos\n",
    "        return fallback\n",
    "    \n",
    "    # Draw red placeholder annotations\n",
    "    for location in model_results['red_placeholders']['locations']:\n",
    "        rx, ry = guess_position(location)\n",
    "        center_x, center_y = int(rx * w), int(ry * h)\n",
    "        # Draw a red circle for red placeholders\n",
    "        cv2.circle(annotated_img, (center_x, center_y), 40, (255, 0, 0), 3)\n",
    "        # Add a small label\n",
    "        cv2.putText(annotated_img, \"R\", (center_x-8, center_y+8), \n",
    "                   font, font_scale, (255, 0, 0), font_thickness)\n",
    "    \n",
    "    # Draw grey placeholder annotations\n",
    "    for location in model_results['grey_placeholders']['locations']:\n",
    "        gx, gy = guess_position(location)\n",
    "        center_x, center_y = int(gx * w), int(gy * h)\n",
    "        # Draw a grey circle for grey placeholders\n",
    "        cv2.circle(annotated_img, (center_x, center_y), 40, (169, 169, 169), 3)\n",
    "        # Add a small label\n",
    "        cv2.putText(annotated_img, \"G\", (center_x-8, center_y+8), \n",
    "                   font, font_scale, (169, 169, 169), font_thickness)\n",
    "    \n",
    "    return annotated_img\n",
    "\n",
    "# Create side-by-side comparisons for a few example images\n",
    "def create_side_by_side_comparison(img_path, gpt_results, llama_results):\n",
    "    # Draw annotations\n",
    "    gpt_img = draw_annotations(img_path, gpt_results, \"GPT-4.1\")\n",
    "    llama_img = draw_annotations(img_path, llama_results, \"Llama 3.2\")\n",
    "    \n",
    "    # Create side-by-side image\n",
    "    comparison = np.hstack((gpt_img, llama_img))\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Create a grid of comparisons for visualization\n",
    "fig, axes = plt.subplots(len(test_images), 1, figsize=(20, 8 * len(test_images)))\n",
    "if len(test_images) == 1:\n",
    "    axes = [axes]  # Make it indexable if only one image\n",
    "\n",
    "for i, img_path in enumerate(test_images):\n",
    "    img_name = os.path.basename(img_path)\n",
    "    img_results = all_results[img_name]\n",
    "    \n",
    "    if 'gpt4' in img_results and 'llama' in img_results:\n",
    "        gpt_results = img_results['gpt4']['parsed']\n",
    "        llama_results = img_results['llama']['parsed']\n",
    "        \n",
    "        # Create comparison image\n",
    "        comparison = create_side_by_side_comparison(img_path, gpt_results, llama_results)\n",
    "        \n",
    "        # Display in the grid\n",
    "        axes[i].imshow(comparison)\n",
    "        axes[i].set_title(f\"Image: {img_name}\", fontsize=16)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "# Adjust layout and save\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/comparison/visual_comparison.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary visualizations and conclusions\n",
    "\n",
    "# 1. Create a summary bar chart comparing model accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Prepare data for summary\n",
    "summary_data = []\n",
    "for model in ['GPT-4.1', 'Llama 3.2']:\n",
    "    model_df = df_comparison[df_comparison['model'] == model]\n",
    "    \n",
    "    total_red = model_df['red_count'].sum()\n",
    "    total_grey = model_df['grey_count'].sum()\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Model': model,\n",
    "        'Red Placeholders': total_red,\n",
    "        'Grey Placeholders': total_grey,\n",
    "        'Total Placeholders': total_red + total_grey\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "# Plot the summary\n",
    "summary_melted = pd.melt(\n",
    "    df_summary,\n",
    "    id_vars=['Model'],\n",
    "    value_vars=['Red Placeholders', 'Grey Placeholders', 'Total Placeholders'],\n",
    "    var_name='Placeholder Type',\n",
    "    value_name='Count'\n",
    ")\n",
    "\n",
    "sns.barplot(\n",
    "    data=summary_melted,\n",
    "    x='Model',\n",
    "    y='Count',\n",
    "    hue='Placeholder Type',\n",
    "    palette=['darkred', 'darkgrey', 'darkblue'],\n",
    "    errorbar=None\n",
    ")\n",
    "\n",
    "plt.title('Total Placeholder Counts by Model', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Total Count', fontsize=14)\n",
    "plt.legend(title='Placeholder Type')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the summary plot\n",
    "plt.savefig('output/comparison/model_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Calculate agreement rate between models\n",
    "model_agreement = []\n",
    "for img in df_comparison['image'].unique():\n",
    "    img_data = df_comparison[df_comparison['image'] == img]\n",
    "    \n",
    "    if len(img_data) >= 2:  # Make sure we have both models\n",
    "        gpt_data = img_data[img_data['model'] == 'GPT-4.1'].iloc[0]\n",
    "        llama_data = img_data[img_data['model'] == 'Llama 3.2'].iloc[0]\n",
    "        \n",
    "        # Check agreement on has_red and has_grey\n",
    "        red_agreement = gpt_data['has_red'] == llama_data['has_red']\n",
    "        grey_agreement = gpt_data['has_grey'] == llama_data['has_grey']\n",
    "        \n",
    "        # Check agreement on counts (within Â±1 tolerance)\n",
    "        red_count_agreement = abs(gpt_data['red_count'] - llama_data['red_count']) <= 1\n",
    "        grey_count_agreement = abs(gpt_data['grey_count'] - llama_data['grey_count']) <= 1\n",
    "        \n",
    "        model_agreement.append({\n",
    "            'Image': img,\n",
    "            'Red Detection Agreement': red_agreement,\n",
    "            'Grey Detection Agreement': grey_agreement,\n",
    "            'Red Count Agreement': red_count_agreement,\n",
    "            'Grey Count Agreement': grey_count_agreement\n",
    "        })\n",
    "\n",
    "# Create agreement DataFrame\n",
    "df_agreement_summary = pd.DataFrame(model_agreement)\n",
    "\n",
    "# Calculate overall agreement percentages\n",
    "agreement_percentages = {\n",
    "    'Red Detection': df_agreement_summary['Red Detection Agreement'].mean() * 100,\n",
    "    'Grey Detection': df_agreement_summary['Grey Detection Agreement'].mean() * 100,\n",
    "    'Red Count': df_agreement_summary['Red Count Agreement'].mean() * 100,\n",
    "    'Grey Count': df_agreement_summary['Grey Count Agreement'].mean() * 100\n",
    "}\n",
    "\n",
    "# Plot agreement percentages\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=list(agreement_percentages.keys()),\n",
    "    y=list(agreement_percentages.values()),\n",
    "    palette='Blues_d'\n",
    ")\n",
    "\n",
    "# Add percentage labels\n",
    "for i, p in enumerate(plt.gca().patches):\n",
    "    plt.text(\n",
    "        p.get_x() + p.get_width()/2.,\n",
    "        p.get_height() + 1,\n",
    "        f'{agreement_percentages[list(agreement_percentages.keys())[i]]:.1f}%',\n",
    "        ha=\"center\", fontsize=12\n",
    "    )\n",
    "\n",
    "plt.title('Agreement Between GPT-4.1 and Llama 3.2', fontsize=16)\n",
    "plt.xlabel('Agreement Type', fontsize=14)\n",
    "plt.ylabel('Agreement Percentage', fontsize=14)\n",
    "plt.ylim(0, 105)  # Set y-axis limit to accommodate the labels\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the agreement plot\n",
    "plt.savefig('output/comparison/model_agreement_percentages.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Generate a summary table with key findings\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "\n",
    "# Create a DataFrame for key metrics\n",
    "key_metrics = [\n",
    "    {'Metric': 'Total Red Placeholders Detected', \n",
    "     'GPT-4.1': df_comparison[df_comparison['model'] == 'GPT-4.1']['red_count'].sum(),\n",
    "     'Llama 3.2': df_comparison[df_comparison['model'] == 'Llama 3.2']['red_count'].sum()},\n",
    "    {'Metric': 'Total Grey Placeholders Detected',\n",
    "     'GPT-4.1': df_comparison[df_comparison['model'] == 'GPT-4.1']['grey_count'].sum(),\n",
    "     'Llama 3.2': df_comparison[df_comparison['model'] == 'Llama 3.2']['grey_count'].sum()},\n",
    "    {'Metric': 'Images with Red Placeholders',\n",
    "     'GPT-4.1': df_comparison[df_comparison['model'] == 'GPT-4.1']['has_red'].sum(),\n",
    "     'Llama 3.2': df_comparison[df_comparison['model'] == 'Llama 3.2']['has_red'].sum()},\n",
    "    {'Metric': 'Images with Grey Placeholders',\n",
    "     'GPT-4.1': df_comparison[df_comparison['model'] == 'GPT-4.1']['has_grey'].sum(),\n",
    "     'Llama 3.2': df_comparison[df_comparison['model'] == 'Llama 3.2']['has_grey'].sum()}]\n",
    "\n",
    "# Create summary DataFrame, display and save\n",
    "df_key_metrics = pd.DataFrame(key_metrics)\n",
    "display(df_key_metrics)\n",
    "df_key_metrics.to_csv('output/comparison/key_metrics.csv', index=False)\n",
    "print(\"\\nResults and visualizations have been saved to the 'output/comparison' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae1890",
   "metadata": {},
   "source": [
    "## 6. Filling Placeholders with Generated Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_placeholders_with_seg_and_dalle(\n",
    "    img_path: str,\n",
    "    prompt: str,\n",
    "    dalle_deployment: str,\n",
    "    api_version: str = \"2024-02-01\",\n",
    "    confidence: float = 0.4\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    1) Runs Roboflow instance segmentation on img_path\n",
    "    2) For each detected mask or polygon, generates a DALLÂ·E patch via Azure\n",
    "    3) Pastes each patch into the original, respecting the region shape\n",
    "    4) Displays and saves the composited image\n",
    "    \"\"\"\n",
    "    # âââ 1. Run Roboflow inference âââââââââââââ\n",
    "    rf_key = os.getenv(\"ROBOFLOW_API_KEY\")\n",
    "    if not rf_key:\n",
    "        raise EnvironmentError(\"ROBOFLOW_API_KEY not set\")\n",
    "\n",
    "    rf = Roboflow(api_key=rf_key)\n",
    "    project = rf.workspace(\"goldbach-neo-testspace\").project(\"stellenbilder\")\n",
    "    model   = project.version(3).model\n",
    "\n",
    "    raw   = model.predict(img_path, confidence=confidence).json()\n",
    "    preds = raw.get(\"predictions\", [])\n",
    "    if not preds:\n",
    "        print(\"No placeholders detected; nothing to fill.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Detected {len(preds)} placeholders\")\n",
    "\n",
    "    # Load original image as RGBA\n",
    "    orig_bgr = cv2.imread(img_path)\n",
    "    orig_rgb = cv2.cvtColor(orig_bgr, cv2.COLOR_BGR2RGB)\n",
    "    h, w, _  = orig_rgb.shape\n",
    "    canvas   = Image.fromarray(orig_rgb).convert(\"RGBA\")\n",
    "\n",
    "    # Prepare DALLÂ·E headers\n",
    "    dalle_key = os.getenv(\"AZURE_API_KEY_DALLE\")\n",
    "    dalle_url = os.getenv(\"AZURE_URL_DALLE\")\n",
    "    if not dalle_key or not dalle_url:\n",
    "        raise EnvironmentError(\"AZURE_API_KEY_DALLE and AZURE_URL_DALLE must be set\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {dalle_key}\",\n",
    "        \"Content-Type\":  \"application/json\"\n",
    "    }\n",
    "    gen_endpoint = (\n",
    "        f\"{dalle_url.rstrip('/')}/openai/deployments/\"\n",
    "        f\"{dalle_deployment}/images/generations?api-version={api_version}\"\n",
    "    )\n",
    "\n",
    "    # âââ 2. Process each prediction ââââââââââââ\n",
    "    for idx, pred in enumerate(preds, 1):\n",
    "        # 2a) Build a binary mask for this region\n",
    "        mask_np = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "        if pred.get(\"mask\"):\n",
    "            # if Roboflow returned an encoded mask\n",
    "            mask_bytes = base64.b64decode(pred[\"mask\"])\n",
    "            mask_img   = Image.open(io.BytesIO(mask_bytes)).convert(\"L\")\n",
    "            mask_np    = np.array(mask_img)\n",
    "\n",
    "        elif pred.get(\"points\") and isinstance(pred[\"points\"], list):\n",
    "            # Roboflow here returns a list of point dicts\n",
    "            pts = np.array(\n",
    "                [[p[\"x\"], p[\"y\"]] for p in pred[\"points\"]],\n",
    "                dtype=np.int32\n",
    "            ).reshape(-1,1,2)\n",
    "            cv2.fillPoly(mask_np, [pts], 255)\n",
    "\n",
    "        else:\n",
    "            # fallback to bbox\n",
    "            cx, cy, bw, bh = pred[\"x\"], pred[\"y\"], pred[\"width\"], pred[\"height\"]\n",
    "            x1 = int(cx - bw/2);  y1 = int(cy - bh/2)\n",
    "            x2 = x1 + int(bw);    y2 = y1 + int(bh)\n",
    "            cv2.rectangle(mask_np, (x1, y1), (x2, y2), 255, -1)\n",
    "\n",
    "        # 2b) Get bounding rect of mask\n",
    "        contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if not contours:\n",
    "            print(f\"  â skipped placeholder #{idx}: no valid mask\")\n",
    "            continue\n",
    "\n",
    "        x1, y1, bw, bh = cv2.boundingRect(max(contours, key=cv2.contourArea))\n",
    "        x2, y2 = x1 + bw, y1 + bh\n",
    "\n",
    "        print(f\"[{idx}/{len(preds)}] Generating patch at ({x1},{y1}) {bw}Ã{bh}\")\n",
    "\n",
    "        # 2c) Call DALLÂ·E to generate a square patch, then resize\n",
    "        payload = {\n",
    "            \"prompt\": prompt,\n",
    "            \"size\":   \"1024x1024\",\n",
    "            \"n\":      1,\n",
    "            \"style\":  \"vivid\",\n",
    "            \"quality\":\"standard\"\n",
    "        }\n",
    "        resp = requests.post(gen_endpoint, headers=headers, json=payload)\n",
    "        resp.raise_for_status()\n",
    "        patch_url = resp.json()[\"data\"][0][\"url\"]\n",
    "\n",
    "        patch = Image.open(io.BytesIO(requests.get(patch_url).content))\n",
    "        patch = patch.resize((bw, bh), Image.LANCZOS).convert(\"RGBA\")\n",
    "\n",
    "        # 2d) Paste with mask crop\n",
    "        mask_crop = mask_np[y1:y2, x1:x2]\n",
    "        mask_img  = Image.fromarray(mask_crop).convert(\"L\")\n",
    "        canvas.paste(patch, (x1, y1), mask_img)\n",
    "\n",
    "        # throttle to reduce rate hits\n",
    "        time.sleep(1.0)\n",
    "\n",
    "    # âââ 3. Show & save âââââââââââââââââââââââ\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Filled placeholders via Segmentation + DALLÂ·E\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    out_file = \"output/filled_with_segmentation.png\"\n",
    "    os.makedirs(os.path.dirname(out_file), exist_ok=True)\n",
    "    canvas.convert(\"RGB\").save(out_file)\n",
    "    print(f\"Saved result to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_placeholders_with_seg_and_dalle(\n",
    "    img_path          = \"Stellenbilder-no-augs/test/66229_jpg.rf.d4ba322d0570bb4e119115e226dcb558.jpg\",\n",
    "    prompt            = \"A luxury perfume advertisement for 'Gold Fragrance Neo', elegant woman in soft golden lighting\",\n",
    "    dalle_deployment  = \"dall-e-3-workshop\",\n",
    "    api_version       = \"2024-02-01\",\n",
    "    confidence        = 0.6\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c-venv-gobn-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
